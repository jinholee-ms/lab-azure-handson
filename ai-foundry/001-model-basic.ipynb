{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca399873",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "본 `ipynb` 은 `Python=3.12` 에서 작성하였습니다. Package dependency 를 해결하기 위해 아래 cell 을 실행해주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fe7d7e",
   "metadata": {},
   "source": [
    "## Install Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8445dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install -U dotenv openai azure-ai-projects azure-monitor-opentelemetry opentelemetry-instrumentation-openai-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e496ba36",
   "metadata": {},
   "source": [
    "## Load environment variables from a .env file\n",
    "secret 노출을 피하고 notebook 들간의 일관된 환경변수를 설정하기 위해 `dotenv` 을 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14085ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT\")\n",
    "AZURE_AI_FOUNDRY_PROJECT_ENDPOINT = os.getenv(\"AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\")\n",
    "AZURE_OPENAI_SECONDARY_ENDPOINT = os.getenv(\"AZURE_OPENAI_SECONDARY_ENDPOINT\")\n",
    "AZURE_OPENAI_SECONDARY_API_KEY = os.getenv(\"AZURE_OPENAI_SECONDARY_API_KEY\")\n",
    "AZURE_OPENAI_SECONDARY_IMAGE_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_SECONDARY_IMAGE_DEPLOYMENT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85fb32e",
   "metadata": {},
   "source": [
    "# Connect azure monitor of AI Foundry project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a8028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input/output message content를 캡처하기 위해 환경변수를 설정합니다.\n",
    "import os\n",
    "\n",
    "os.environ[\"OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT\"] = \"true\"\n",
    "os.environ[\"AZURE_TRACING_GEN_AI_CONTENT_RECORDING_ENABLED\"] = \"true\"\n",
    "\n",
    "# from azure.ai.projects import AIProjectClient\n",
    "from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "from opentelemetry.instrumentation.openai_v2 import OpenAIInstrumentor\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import AzureCliCredential\n",
    "\n",
    "# OpenAI API 호출을 자동으로 계측하기 위해 OpenAIInstrumentor를 사용합니다.\n",
    "OpenAIInstrumentor().instrument()\n",
    "\n",
    "# AIProjectClient를 사용하여 Azure AI Foundry 프로젝트에 연결합니다.\n",
    "# API Key를 TokenCredential로 변환합니다.\n",
    "project_client = AIProjectClient(\n",
    "    credential=AzureCliCredential(),\n",
    "    endpoint=AZURE_AI_FOUNDRY_PROJECT_ENDPOINT,\n",
    ")\n",
    "# Azure Monitor를 구성합니다.\n",
    "configure_azure_monitor(connection_string=project_client.telemetry.get_application_insights_connection_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca6acc8",
   "metadata": {},
   "source": [
    "# OpenAI Generative APIs\n",
    "OpenAI 는 Frontier LLM 모델을 서비스한다. Azure 는 OpenAI 의 동일한 모델들과 APIs 를 서비스하고 있으며, Python 에서는 `openai` package 를 통해서 Azure OpenAI (AOAI) 의 client 를 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024c1cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00416a13",
   "metadata": {},
   "source": [
    "## Simply Q&A Behavior\n",
    "가장 초기에 나온 chat completions API 를 통해 question 에 대한 answer 를 synthesis 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd2a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [required] 꼭 gpt-4.1 모델이 Azure OpenAI 에서 배포되어 있어야 한다.\n",
    "r = client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"GPT 를 쓸때 비용을 절감할 수 있는 방법은 어떤게 있을까 ?\"}],\n",
    ")\n",
    "\n",
    "print(\"=== Response Message ===\")\n",
    "print(r.choices[0].message.content)\n",
    "print(\"\\n=== Response Usage ===\")\n",
    "print(r.usage.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2081f20",
   "metadata": {},
   "source": [
    "## Advanced Q&A using Assistants API\n",
    "Assistants API 는 Conversational API 이다. 단순 answer 를 생성해내는 것을 넘어 사용자와 모델의 대화를 제어할 수 있다. 여전히 beta 이고 벌써 deprecated 된단다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6812c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assistants API 를 통해 Q&A 를 수행합니다.\n",
    "assistant = client.beta.assistants.create(model=\"gpt-4.1\")\n",
    "\n",
    "thread = client.beta.threads.create()\n",
    "client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"GPT 를 쓸때 비용을 절감할 수 있는 방법은 어떤게 있을까 ?\",\n",
    ")\n",
    "\n",
    "run = client.beta.threads.runs.create(thread_id=thread.id, assistant_id=assistant.id)\n",
    "\n",
    "# 폴링 (간단 예시)\n",
    "while True:\n",
    "    run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
    "    if run.status == \"completed\":\n",
    "        break\n",
    "\n",
    "messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "print(\"=== Response Message ===\")\n",
    "print(messages.data[0].content[0].text.value)\n",
    "print(\"\\n=== Run Usage ===\")\n",
    "print(run.usage.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3b6abb",
   "metadata": {},
   "source": [
    "## Other approach Q&A using Responses API\n",
    "가장 최신에 공개된 응답형 API 이다. 각 dialog 마다 `response id` 가 주어지고 이를 통해 conversation history 를 제어할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43934a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Responses API 를 통해 Q&A 를 수행합니다.\n",
    "r = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=[{\"role\": \"user\", \"content\": \"GPT 를 쓸때 비용을 절감할 수 있는 방법은 어떤게 있을까 ?\"}],\n",
    "    previous_response_id=None,\n",
    ")\n",
    "\n",
    "print(\"=== Response Message ===\")\n",
    "print(r.output_text)\n",
    "print(\"\\n=== Response Usage ===\")\n",
    "print(r.usage.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07c7192",
   "metadata": {},
   "source": [
    "## Batch processing with Files API\n",
    "offline processing 으로 Batch 를 사용해보자. Batch 에 전달된 input data 는 Files API 를 사용해도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb87695",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile resources/batch_inputs.jsonl\n",
    "{\"custom_id\": \"1\", \"method\": \"POST\", \"url\": \"/chat/completions\", \"body\": {\"model\": \"gpt-4o-batch\", \"messages\": [{\"role\": \"user\", \"content\": \"봄에 가기 좋은 여행 장소 추천해줘.\"}]}}\n",
    "{\"custom_id\": \"2\", \"method\": \"POST\", \"url\": \"/chat/completions\", \"body\": {\"model\": \"gpt-4o-batch\", \"messages\": [{\"role\": \"user\", \"content\": \"여름에 가기 좋은 여행 장소 추천해줘.\"}]}}\n",
    "{\"custom_id\": \"3\", \"method\": \"POST\", \"url\": \"/chat/completions\", \"body\": {\"model\": \"gpt-4o-batch\", \"messages\": [{\"role\": \"user\", \"content\": \"가을에 가기 좋은 여행 장소 추천해줘.\"}]}}\n",
    "{\"custom_id\": \"4\", \"method\": \"POST\", \"url\": \"/chat/completions\", \"body\": {\"model\": \"gpt-4o-batch\", \"messages\": [{\"role\": \"user\", \"content\": \"겨울에 가기 좋은 여행 장소 추천해줘.\"}]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefa0558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch 파일 업로드\n",
    "file = client.files.create(\n",
    "    file=open(\"batch_inputs.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\",\n",
    "    extra_body={\"expires_after\": {\"seconds\": 3600, \"anchor\": \"created_at\"}}, # 1시간 후에 만료됩니다.\n",
    ")\n",
    "\n",
    "# batch 요청\n",
    "response = client.batches.create(\n",
    "    input_file_id=file.id,\n",
    "    endpoint=\"/chat/completions\",\n",
    "    completion_window=\"12h\",    # 12시간 동안 요청을 수신합니다.\n",
    "    extra_body={\"expires_after\": {\"seconds\": 3600, \"anchor\": \"created_at\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91109f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# 배치 상태 확인\n",
    "status = \"validating\"\n",
    "while status not in (\"completed\", \"failed\", \"canceled\"):\n",
    "    response = client.batches.retrieve(response.id)\n",
    "    status = response.status\n",
    "    print(f\"{datetime.datetime.now()} Batch Id: {response.id},  Status: {status}\")\n",
    "    time.sleep(10)\n",
    "\n",
    "if response.status == \"failed\":\n",
    "    for error in response.errors.data:  \n",
    "        print(f\"Error code {error.code} Message {error.message}\")\n",
    "\n",
    "# 결과 파일 다운로드\n",
    "output_file_id = response.output_file_id\n",
    "if not response.output_file_id:\n",
    "    output_file_id = response.error_file_id\n",
    "\n",
    "if output_file_id:\n",
    "    file_response = client.files.content(output_file_id)\n",
    "    for rawdata in file_response.text.strip().split(\"\\n\"):\n",
    "        print(json.loads(rawdata))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7368caab",
   "metadata": {},
   "source": [
    "## Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0948c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from openai import OpenAI\n",
    "\n",
    "img = OpenAI(\n",
    "    base_url=AZURE_OPENAI_SECONDARY_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_SECONDARY_API_KEY,\n",
    ").images.generate(\n",
    "    model=AZURE_OPENAI_SECONDARY_IMAGE_DEPLOYMENT,\n",
    "    prompt=\"대한민국 국기를 그려줘.\",\n",
    "    n=1,\n",
    "    size=\"1024x1024\",\n",
    ")\n",
    "\n",
    "# 이미지 저장\n",
    "image_bytes = base64.b64decode(img.data[0].b64_json)\n",
    "with open(\"gpt-image-1.png\", \"wb\") as f:\n",
    "    f.write(image_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6330f7",
   "metadata": {},
   "source": [
    "## Multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503cf6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.chat import (\n",
    "    ChatCompletionContentPartTextParam,\n",
    "    ChatCompletionContentPartImageParam,\n",
    "    ChatCompletionUserMessageParam,\n",
    ")\n",
    "\n",
    "with open(\"gpt-image-1.png\", \"rb\") as file:\n",
    "    image = base64.b64encode(file.read()).decode(\"utf-8\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=AZURE_OPENAI_CHAT_DEPLOYMENT,\n",
    "    messages=[\n",
    "        ChatCompletionUserMessageParam(\n",
    "            role=\"user\",\n",
    "            content=[\n",
    "                ChatCompletionContentPartTextParam(type=\"text\", text=\"이 사진에 대해 설명해줘.\"),\n",
    "                ChatCompletionContentPartImageParam(\n",
    "                    type=\"image_url\",\n",
    "                    image_url={\"url\": \"data:image/png;base64,\" + image},\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-lab-azure-handson-ai-foundry-001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
