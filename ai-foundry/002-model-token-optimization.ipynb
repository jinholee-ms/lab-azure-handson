{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca399873",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "본 `ipynb` 은 `Python=3.12` 에서 작성하였습니다. Package dependency 를 해결하기 위해 아래 cell 을 실행해주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fe7d7e",
   "metadata": {},
   "source": [
    "## Install Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8445dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install -U dotenv openai azure-ai-projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e496ba36",
   "metadata": {},
   "source": [
    "## Load environment variables from a .env file\n",
    "secret 노출을 피하고 notebook 들간의 일관된 환경변수를 설정하기 위해 `dotenv` 을 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14085ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca6acc8",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "OpenAI 는 다양한 최적화 도구를 제공한다. 이를 통해 성능과 비용을 선택적으로 개선해 나갈 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024c1cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00416a13",
   "metadata": {},
   "source": [
    "## Input Token Caching\n",
    "흔희 많이 쓰이는 건 input token 캐시이다. 비용 측면에서도 50% 정도 절감할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd2a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.chat import ChatCompletionUserMessageParam\n",
    "\n",
    "# chat completions API 호출\n",
    "response = client.chat.completions.create(\n",
    "    model=AZURE_OPENAI_CHAT_DEPLOYMENT,\n",
    "    messages=[\n",
    "        ChatCompletionUserMessageParam(\n",
    "            role=\"user\",\n",
    "            content=\"입력되는 패턴의 숫자를 이어서 100개 더 출력해줘.\\n\" + \"\\n\".join([f\"{i}\" for i in range(1000)]),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.usage.model_dump_json(indent=2))\n",
    "\n",
    "# responses API 호출\n",
    "previous_response_id = None\n",
    "for season in [\"봄\", \"여름\", \"가을\", \"겨울\"]:\n",
    "    response = client.responses.create(\n",
    "        model=AZURE_OPENAI_CHAT_DEPLOYMENT,\n",
    "        input=f\"{season} 계절에 잘 어울리는 옷을 추천해줘.\",\n",
    "        previous_response_id=previous_response_id,\n",
    "    )\n",
    "    print(f\"--- {season} ---\")\n",
    "    print(response.usage.model_dump_json(indent=2))\n",
    "    previous_response_id = response.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b219f160",
   "metadata": {},
   "source": [
    "## Structed Outputs\n",
    "출력되는 token 량은 LLM 모델이 non-deterministic 하기에 요청마다 다르다. Output Schema 를 정의하여 응답 format 으로 전달하면 해당 구조에 맞게 generate 한다. 모델에 따라 기대하는 token usage 가 달라질 수 있기에 이점 유념하도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb4888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class IntentEvent(BaseModel):\n",
    "    intent: str\n",
    "\n",
    "# LLM 모델인 gpt-4.1 을 사용하여 한국어로 된 문장의 의도를 분석하고, structed output 으로 반환받아보자.\n",
    "# gpt-4.1 은 intent 에 해당하는 token 만 generate 한다.\n",
    "response = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4.1\", # replace with the model deployment name of your gpt-4o 2024-08-06 deployment\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"너는 대화 의도 분석을 전문으로 하는 AI야.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Alice와 Bob이 금요일에 과학 박람회에 가기로 했어.\"},\n",
    "    ],\n",
    "    response_format=IntentEvent,\n",
    ")\n",
    "print(\"---- gpt-4.1 ----\")\n",
    "print(f\"응답: {response.choices[0].message.parsed}\")\n",
    "print(response.usage.model_dump_json(indent=2))\n",
    "\n",
    "# LLM 모델인 gpt-5 을 사용하여 한국어로 된 문장의 의도를 분석하고, structed output 으로 반환받아보자.\n",
    "# gpt-5 은 gpt-4.1 보다 많은 token 을 generate 하고, 후처리로 intent 를 추출한다.\n",
    "response = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-5\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"너는 대화 의도 분석을 전문으로 하는 AI야.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Alice와 Bob이 금요일에 과학 박람회에 가기로 했어.\"},\n",
    "    ],\n",
    "    response_format=IntentEvent,\n",
    ")\n",
    "print(\"---- gpt-5 ----\")\n",
    "print(f\"응답: {response.choices[0].message.parsed}\")\n",
    "print(response.usage.model_dump_json(indent=2))\n",
    "\n",
    "# responses API 를 사용해보자. 동일하지만...\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-5\",\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": \"너는 대화 의도 분석을 전문으로 하는 AI야.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Alice와 Bob이 금요일에 과학 박람회에 가기로 했어.\"},\n",
    "    ],\n",
    "    text_format=IntentEvent,\n",
    ")\n",
    "print(\"---- gpt-5 ----\")\n",
    "print(f\"응답: {response.output_text}\")\n",
    "print(response.usage.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2081f20",
   "metadata": {},
   "source": [
    "## Predicted Output\n",
    "Assistants API 는 Conversational API 이다. 단순 answer 를 생성해내는 것을 넘어 사용자와 모델의 대화를 제어할 수 있다. 여전히 beta 이고 벌써 deprecated 된단다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6812c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from openai.types.chat import ChatCompletionUserMessageParam, ChatCompletionPredictionContentParam\n",
    "\n",
    "\n",
    "code = \"\"\"\n",
    "In the field of artificial intelligence, particularly in the development of large language models, the ability to generate coherent, contextually relevant, and semantically accurate responses over extended conversations has become not only a benchmark for performance but also a critical factor in determining the practical applicability of these models in real-world scenarios, including customer support automation, academic research assistance, and interactive educational tools.\n",
    "\"\"\"\n",
    "refactor_prompt = \"\"\"\n",
    "Replace the \"artificial intelligence\" with an \"AI\".\n",
    "\"\"\"\n",
    "\n",
    "start_time = time.time()\n",
    "completion = client.chat.completions.create(\n",
    "    model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"),\n",
    "    messages=[\n",
    "        ChatCompletionUserMessageParam(role=\"user\", content=refactor_prompt),\n",
    "        ChatCompletionUserMessageParam(role=\"user\", content=code),\n",
    "    ],\n",
    "    prediction=ChatCompletionPredictionContentParam(type=\"content\", content=code),\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")\n",
    "print(completion.choices[0].message.content)\n",
    "\n",
    "print(completion.model_dump_json(indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-lab-azure-handson-ai-foundry-001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
