{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Install Python packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fed5e1f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip -q install -U azure-ai-ml azure-identity datasets==4.0.0 azureml-mlflow mlflow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "259e9fc1",
      "metadata": {},
      "source": [
        "# Load environment variables from a .env file\n",
        "secret 노출을 피하고 notebook 들간의 일관된 환경변수를 설정하기 위해 `dotenv` 을 이용한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d425fc04",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(override=True)\n",
        "\n",
        "AZURE_AML_SUBSCRIPTION_ID = os.getenv(\"AZURE_AML_SUBSCRIPTION_ID\")\n",
        "AZURE_AML_RESOURCE_GROUP = os.getenv(\"AZURE_AML_RESOURCE_GROUP\")\n",
        "AZURE_AML_WORKSPACE = os.getenv(\"AZURE_AML_WORKSPACE\")\n",
        "AZURE_AML_CLUSTER = os.getenv(\"AZURE_AML_CLUSTER\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f1234eb",
      "metadata": {},
      "source": [
        "# Create a Azure Machine Learning cilent\n",
        "Azure Machine Learning 의 Client 객체인 `MLClient` 을 생성한다. 본 예제는 Azure CLI 로그인 Credential 을 사용하고 있다. 터미널에서 `az login` 을 정상적으로 완료하여야 한다. `az` 명령어가 설치되어 있지 않다면 [Azure CLI 설치하는 방법](https://learn.microsoft.com/ko-kr/cli/azure/install-azure-cli?view=azure-cli-latest) 을 참고한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0354ecfc",
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.identity import AzureCliCredential\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "ml_client = MLClient(\n",
        "    AzureCliCredential(),\n",
        "    AZURE_AML_SUBSCRIPTION_ID,\n",
        "    AZURE_AML_RESOURCE_GROUP,\n",
        "    AZURE_AML_WORKSPACE,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91b1d85a",
      "metadata": {},
      "source": [
        "# Model Fine Tuning\n",
        "OpenAI 에서 공개한 오픈웨이트 모델인 gpt-oss 를 사용한다. AML model registry 에 등록되어 있는 모델을 사용하여 fine tuning 을 해본다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27160264",
      "metadata": {},
      "source": [
        "## Search gpt-oss-20b model\n",
        "먼저 gpt-oss-20b 를 조회해본다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "368fdd93",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = \"gpt-oss-20b\"\n",
        "model = MLClient(\n",
        "    AzureCliCredential(),\n",
        "    registry_name=\"azureml-openai-oss\",\n",
        ").models.get(model_name, label=\"latest\")\n",
        "model.properties[\"finetune-recommended-sku\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e148aa2",
      "metadata": {},
      "source": [
        "## Preparing Dataset\n",
        "huggingface 의 `HuggingFaceH4/Multilingual-Thinking` 을 train/valid/test 용 dataset 으로 준비한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ddccf02",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "def save_hf_dataset_to_jsonl(dataset, output_file):\n",
        "    \"\"\"Save HuggingFace dataset to JSONL file.\"\"\"\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for record in dataset:\n",
        "            f.write(json.dumps(record, ensure_ascii=False, default=str) + \"\\n\")\n",
        "    print(f\"Saved {len(dataset)} records to {output_file}\")\n",
        "\n",
        "\n",
        "def convert_multilingual_thinking(input_file, output_file):\n",
        "    \"\"\"Convert multilingual thinking dataset to standard JSONL format with only messages field.\"\"\"\n",
        "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "    json_objects = [json.loads(line) for line in lines if line.strip()]\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for obj in json_objects:\n",
        "            cleaned_messages = []\n",
        "            for msg in obj.get(\"messages\", []):\n",
        "                cleaned_msg = {\"role\": msg[\"role\"], \"content\": msg[\"content\"]}\n",
        "                if \"thinking\" in msg:\n",
        "                    cleaned_msg[\"thinking\"] = msg[\"thinking\"]\n",
        "                cleaned_messages.append(cleaned_msg)\n",
        "            output_obj = {\"messages\": cleaned_messages}\n",
        "            f.write(json.dumps(output_obj, ensure_ascii=False) + \"\\n\")\n",
        "    print(f\"Converted {len(json_objects)} records to {output_file}\")\n",
        "\n",
        "\n",
        "def split_jsonl(\n",
        "    input_file,\n",
        "    train_file,\n",
        "    valid_file,\n",
        "    test_file,\n",
        "    train_size=800,\n",
        "    valid_size=100,\n",
        "    test_size=100,\n",
        "    seed=42,\n",
        "):\n",
        "    \"\"\"Split JSONL file into train/valid/test splits.\"\"\"\n",
        "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = [line for line in f if line.strip()]\n",
        "    random.seed(seed)\n",
        "    random.shuffle(lines)\n",
        "    train_lines = lines[:train_size]\n",
        "    valid_lines = lines[train_size : train_size + valid_size]\n",
        "    test_lines = lines[train_size + valid_size : train_size + valid_size + test_size]\n",
        "    with open(train_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.writelines(train_lines)\n",
        "    with open(valid_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.writelines(valid_lines)\n",
        "    with open(test_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.writelines(test_lines)\n",
        "    print(\n",
        "        f\"Train: {len(train_lines)}, Valid: {len(valid_lines)}, Test: {len(test_lines)}\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Usage example:\n",
        "dataset = load_dataset(\"HuggingFaceH4/Multilingual-Thinking\", split=\"train\")\n",
        "save_hf_dataset_to_jsonl(dataset, \"multilingual_thinking_raw_data.json\")\n",
        "convert_multilingual_thinking(\n",
        "    \"multilingual_thinking_raw_data.json\", \"multilingual_thinking_formatted_data.jsonl\"\n",
        ")\n",
        "split_jsonl(\n",
        "    \"multilingual_thinking_formatted_data.jsonl\",\n",
        "    \"multilingual_thinking_train.jsonl\",\n",
        "    \"multilingual_thinking_valid.jsonl\",\n",
        "    \"multilingual_thinking_test.jsonl\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02e56f41",
      "metadata": {},
      "source": [
        "데이터가 어떻게 생겼는지 확인해보자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54fa3493",
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the ./multilingual_thinking_train.jsonl file into a pandas dataframe and show the first row\n",
        "import pandas as pd\n",
        "\n",
        "pd.set_option(\n",
        "    \"display.max_colwidth\", 0\n",
        ")  # set the max column width to 0 to display the full text\n",
        "df = pd.read_json(\"./multilingual_thinking_train.jsonl\", lines=True)\n",
        "df.head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06b85995",
      "metadata": {},
      "source": [
        "## Train model\n",
        "본격적으로 모델을 훈련한다. 아래 compute cluster 와 gpus 개수는 설정한 cluster 에 맞게 수정한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48e2df34",
      "metadata": {},
      "outputs": [],
      "source": [
        "compute_cluster = AZURE_AML_CLUSTER\n",
        "gpus_per_node = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0943e5ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "# Default training parameters\n",
        "training_parameters = dict(\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    learning_rate=5e-6,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        ")\n",
        "# Default optimization parameters\n",
        "optimization_parameters = dict(\n",
        "    apply_lora=\"true\",\n",
        "    apply_deepspeed=\"true\",\n",
        "    deepspeed_stage=3,\n",
        ")\n",
        "# Let's construct finetuning parameters using training and optimization paramters.\n",
        "finetune_parameters = {**training_parameters, **optimization_parameters}\n",
        "\n",
        "# Each model finetuning works best with certain finetuning parameters which are packed with model as `model_specific_defaults`.\n",
        "# Let's override the finetune_parameters in case the model has some custom defaults.\n",
        "if \"model_specific_defaults\" in model.tags:\n",
        "    print(\"Warning! Model specific defaults exist. The defaults could be overridden.\")\n",
        "    finetune_parameters.update(\n",
        "        ast.literal_eval(  # convert string to python dict\n",
        "            model.tags[\"model_specific_defaults\"]\n",
        "        )\n",
        "    )\n",
        "print(\n",
        "    f\"The following finetune parameters are going to be set for the run: {finetune_parameters}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96fd15fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the pipeline display name for distinguishing different runs from the name\n",
        "def get_pipeline_display_name():\n",
        "    batch_size = (\n",
        "        int(finetune_parameters.get(\"per_device_train_batch_size\", 1))\n",
        "        * int(finetune_parameters.get(\"gradient_accumulation_steps\", 1))\n",
        "        * int(gpus_per_node)\n",
        "        * int(finetune_parameters.get(\"num_nodes_finetune\", 1))\n",
        "    )\n",
        "    scheduler = finetune_parameters.get(\"lr_scheduler_type\", \"linear\")\n",
        "    deepspeed = finetune_parameters.get(\"apply_deepspeed\", \"false\")\n",
        "    ds_stage = finetune_parameters.get(\"deepspeed_stage\", \"2\")\n",
        "    if deepspeed == \"true\":\n",
        "        ds_string = f\"ds{ds_stage}\"\n",
        "    else:\n",
        "        ds_string = \"nods\"\n",
        "    lora = finetune_parameters.get(\"apply_lora\", \"false\")\n",
        "    if lora == \"true\":\n",
        "        lora_string = \"lora\"\n",
        "    else:\n",
        "        lora_string = \"nolora\"\n",
        "    save_limit = finetune_parameters.get(\"save_total_limit\", -1)\n",
        "    seq_len = finetune_parameters.get(\"max_seq_length\", -1)\n",
        "    return (\n",
        "        model_name\n",
        "        + \"-\"\n",
        "        + \"reasoning\"\n",
        "        + \"-\"\n",
        "        + f\"bs{batch_size}\"\n",
        "        + \"-\"\n",
        "        + f\"{scheduler}\"\n",
        "        + \"-\"\n",
        "        + ds_string\n",
        "        + \"-\"\n",
        "        + lora_string\n",
        "        + f\"-save_limit{save_limit}\"\n",
        "        + f\"-seqlen{seq_len}\"\n",
        "    )\n",
        "\n",
        "\n",
        "pipeline_display_name = get_pipeline_display_name()\n",
        "print(f\"Display name used for the run: {pipeline_display_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31f9d3b1",
      "metadata": {},
      "source": [
        "## Create a training pipeline and submit it\n",
        "AML 의 `pipeline` 기능을 통해 job 을 생성하여 AML 에 제출한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1eef2f68",
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml.dsl import pipeline\n",
        "from azure.ai.ml import Input\n",
        "\n",
        "# fetch the pipeline component\n",
        "registry_ml_client = MLClient(AzureCliCredential(), registry_name=\"azureml\")\n",
        "pipeline_component_func = registry_ml_client.components.get(\n",
        "    name=\"chat_completion_pipeline\", label=\"latest\"\n",
        ")\n",
        "\n",
        "\n",
        "# define the pipeline job\n",
        "@pipeline(name=pipeline_display_name)\n",
        "def create_pipeline():\n",
        "    chat_completion_pipeline = pipeline_component_func(\n",
        "        pytorch_model_path=model .id,\n",
        "        compute_model_import=compute_cluster,\n",
        "        compute_preprocess=compute_cluster,\n",
        "        compute_finetune=compute_cluster,\n",
        "        compute_model_evaluation=compute_cluster,\n",
        "        # map the dataset splits to parameters\n",
        "        train_file_path=Input(\n",
        "            type=\"uri_file\", path=\"./multilingual_thinking_train.jsonl\"\n",
        "        ),\n",
        "        validation_file_path=Input(\n",
        "            type=\"uri_file\", path=\"./multilingual_thinking_valid.jsonl\"\n",
        "        ),\n",
        "        test_file_path=Input(\n",
        "            type=\"uri_file\", path=\"./multilingual_thinking_test.jsonl\"\n",
        "        ),\n",
        "        # Training settings\n",
        "        number_of_gpu_to_use_finetuning=gpus_per_node,  # set to the number of GPUs available in the compute\n",
        "        **finetune_parameters\n",
        "    )\n",
        "    return {\n",
        "        # map the output of the fine tuning job to the output of pipeline job so that we can easily register the fine tuned model\n",
        "        # registering the model is required to deploy the model to an online or batch endpoint\n",
        "        \"trained_model\": chat_completion_pipeline.outputs.mlflow_model_folder\n",
        "    }\n",
        "\n",
        "\n",
        "pipeline_object = create_pipeline()\n",
        "\n",
        "# don't use cached results from previous jobs\n",
        "pipeline_object.settings.force_rerun = True\n",
        "\n",
        "# set continue on step failure to False\n",
        "pipeline_object.settings.continue_on_step_failure = False\n",
        "\n",
        "# submit the pipeline job\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline_object, experiment_name=\"chat_completion_gpt_oss\"\n",
        ")\n",
        "# wait for the pipeline job to complete\n",
        "ml_client.jobs.stream(pipeline_job.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5bda647",
      "metadata": {},
      "source": [
        "## Register model from the job output\n",
        "job 의 output 으로부터 model 을 찾아 이를 `Model` 로 등록한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be707063",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "from azure.ai.ml.entities import Model\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "# check if the `trained_model` output is available\n",
        "print(\"pipeline job outputs: \", ml_client.jobs.get(pipeline_job.name).outputs)\n",
        "\n",
        "# fetch the model from pipeline job output - not working, hence fetching from fine tune child job\n",
        "model_path_from_job = \"azureml://jobs/{0}/outputs/{1}\".format(\n",
        "    pipeline_job.name, \"trained_model\"\n",
        ")\n",
        "\n",
        "finetuned_model_name = model_name + \"multi-lingual-reasoning\"\n",
        "finetuned_model_name = finetuned_model_name.replace(\"/\", \"-\")\n",
        "print(\"path to register model: \", model_path_from_job)\n",
        "\n",
        "prepare_to_register_model = Model(\n",
        "    path=model_path_from_job,\n",
        "    type=AssetTypes.MLFLOW_MODEL,\n",
        "    name=finetuned_model_name,\n",
        "    version=str(int(time.time())),  # use timestamp as version to avoid version conflict\n",
        "    description=model_name\n",
        "    + \" fine tuned model for multi-lingual-thinking chat-completion\",\n",
        ")\n",
        "print(\"prepare to register model: \\n\", prepare_to_register_model)\n",
        "# register the model from pipeline job output\n",
        "registered_model = ml_client.models.create_or_update(\n",
        "    prepare_to_register_model\n",
        ")\n",
        "print(\"registered model: \\n\", registered_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67f9a73f",
      "metadata": {},
      "source": [
        "# Model Deployment\n",
        "모델을 학습했으니, 마지막으로 모델 배포다. 모델 배포 모듈은 Endpoint 와 Deployment 로 구성된다. Endpoint 는 model deployment 에 대한 reverse proxy 로, 권한&인증/traffic 관리/모니터링 등의 기능을 가진다. Deployment 는 모델을 안정적으로 배포하는 환경을 구축하며, 모델에 따른 다양한 runtime 환경을 지원한다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8543ffbd",
      "metadata": {},
      "source": [
        "## Create a managed online endpoint\n",
        "먼저 endpoint 를 배포해보자. 실시간 추론을 지원하는 online type 으로 배포한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f1b1c59",
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import (\n",
        "    ManagedOnlineEndpoint,\n",
        "    ManagedOnlineDeployment,\n",
        "    ProbeSettings,\n",
        "    OnlineRequestSettings,\n",
        ")\n",
        "\n",
        "# Create online endpoint - endpoint names need to be unique in a region, hence using timestamp to create unique endpoint name\n",
        "\n",
        "online_endpoint_name = \"chat-completion-\" + str(int(time.time()))\n",
        "# create an online endpoint\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name=online_endpoint_name,\n",
        "    auth_mode=\"key\",\n",
        ")\n",
        "ml_client.begin_create_or_update(endpoint).wait()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e188309a",
      "metadata": {},
      "source": [
        "## Create a online deployment\n",
        "다음은 Deployment 이다. Endpoint 와 동일하게 실시간 추론 타입으로 배포하자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "400ac320",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "demo_deployment = ManagedOnlineDeployment(\n",
        "    name=\"demo\",\n",
        "    endpoint_name=online_endpoint_name,\n",
        "    model=registered_model.id,\n",
        "    instance_type=\"Standard_NC40ads_H100_v5\",\n",
        "    instance_count=1,\n",
        "    liveness_probe=ProbeSettings(initial_delay=600),\n",
        "    request_settings=OnlineRequestSettings(request_timeout_ms=90000),\n",
        ")\n",
        "ml_client.online_deployments.begin_create_or_update(demo_deployment).wait()\n",
        "endpoint.traffic = {\"demo\": 100}\n",
        "ml_client.begin_create_or_update(endpoint).result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3daa9b01",
      "metadata": {},
      "outputs": [],
      "source": [
        "# read ./multilingual_thinking_test.jsonl into a pandas dataframe\n",
        "test_df = pd.read_json(\"./multilingual_thinking_test.jsonl\", lines=True)\n",
        "# take few random samples\n",
        "test_df = test_df.sample(n=1)\n",
        "# rebuild index\n",
        "test_df.reset_index(drop=True, inplace=True)\n",
        "test_df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97fb051b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "test_df[\"messages\"][0]\n",
        "messages = []\n",
        "for msg in test_df[\"messages\"][0]:\n",
        "    messages.append(msg)\n",
        "    if msg[\"role\"] == \"user\":\n",
        "        break\n",
        "\n",
        "# create a json object with the key as \"input_data\" and value as a list of values from the text column of the test dataframe\n",
        "parameters = {\n",
        "    \"temperature\": 0.6,\n",
        "    \"top_p\": 0.9,\n",
        "    \"do_sample\": True,\n",
        "    \"max_new_tokens\": 200,\n",
        "}\n",
        "test_json = {\n",
        "    \"input_data\": {\n",
        "        \"input_string\": [messages],\n",
        "        \"parameters\": parameters,\n",
        "    },\n",
        "    \"params\": {},\n",
        "}\n",
        "# save the json object to a file named sample_score.json in the ./samsum-dataset folder\n",
        "with open(\"./sample_score.json\", \"w\") as f:\n",
        "    json.dump(test_json, f)\n",
        "    \n",
        "# score the sample_score.json file using the online endpoint with the azureml endpoint invoke method\n",
        "response = ml_client.online_endpoints.invoke(\n",
        "    endpoint_name=online_endpoint_name,\n",
        "    deployment_name=\"demo\",\n",
        "    request_file=\"./sample_score.json\",\n",
        ")\n",
        "print(\"raw response: \\n\", response, \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7379d728",
      "metadata": {},
      "outputs": [],
      "source": [
        "ml_client.online_endpoints.begin_delete(name=online_endpoint_name).wait()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env-lab-azure-handson-machine-learning-001",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
